{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrG7S1X4H-Oz",
        "outputId": "9b396ea7-35e6-4bed-afba-33b8964664c2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HybridAutoencoder(\n",
              "  (encoder): Sequential(\n",
              "    (0): Linear(in_features=8, out_features=10, bias=True)\n",
              "    (1): LeakyReLU(negative_slope=0.5)\n",
              "    (2): Linear(in_features=10, out_features=8, bias=True)\n",
              "  )\n",
              "  (decoder): Sequential(\n",
              "    (0): Linear(in_features=8, out_features=10, bias=True)\n",
              "    (1): LeakyReLU(negative_slope=0.5)\n",
              "    (2): Linear(in_features=10, out_features=8, bias=True)\n",
              "    (3): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "class HybridAutoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(HybridAutoencoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(8, 10),\n",
        "            nn.LeakyReLU(negative_slope=0.5),\n",
        "            nn.Linear(10, 8)\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(8, 10),\n",
        "            nn.LeakyReLU(negative_slope=0.5),\n",
        "            nn.Linear(10, 8),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded\n",
        "model = HybridAutoencoder()\n",
        "\n",
        "model_path = '/content/drive/MyDrive/SecurityApplicationProject/hybrid_autoencoder_parameters.pth'\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_input = torch.tensor([[0, 1, 0, 1, 1, 0, 1, 0]]).float()\n",
        "with torch.no_grad():\n",
        "    encrypted_vector = model.encoder(test_input)\n",
        "    print('Encrypted vector:', encrypted_vector.numpy())\n",
        "\n",
        "\n",
        "decrypted_vector = model.decoder(encrypted_vector)\n",
        "binarized_output = torch.where(decrypted_vector > 0.5, torch.tensor(1.0), torch.tensor(0.0))\n",
        "print('Binarized output:', binarized_output)\n",
        "\n",
        "correct_predictions = (binarized_output == test_input).float()\n",
        "accuracy = correct_predictions.mean()\n",
        "print(f'Accuracy: {accuracy.item() * 100}%')\n",
        "print(f'Correct predictions: {correct_predictions.sum().item()} out of {test_input.nelement()}')\n",
        "comparison = torch.cat((test_input, binarized_output, correct_predictions), 0)\n",
        "print('Row 1: Original Input\\nRow 2: Binarized Output\\nRow 3: Correct Predictions (1: Correct, 0: Incorrect)\\n', comparison)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtGetk_CK8_O",
        "outputId": "4219fafb-bcd4-4097-b7d9-301e1eb8e2f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encrypted vector: [[ 0.02202405  1.4976857  -1.5635588  -2.3500974  -1.4465282   9.93152\n",
            "  -3.9664261  -3.7735186 ]]\n",
            "Binarized output: tensor([[0., 1., 0., 1., 1., 0., 1., 0.]])\n",
            "Accuracy: 100.0%\n",
            "Correct predictions: 8.0 out of 8\n",
            "Row 1: Original Input\n",
            "Row 2: Binarized Output\n",
            "Row 3: Correct Predictions (1: Correct, 0: Incorrect)\n",
            " tensor([[0., 1., 0., 1., 1., 0., 1., 0.],\n",
            "        [0., 1., 0., 1., 1., 0., 1., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(model_path):\n",
        "    model = HybridAutoencoder()\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "def interactive_test(model):\n",
        "    try:\n",
        "        user_input = input(\"Enter an 8-bit binary sequence (e.g., 01010101): \")\n",
        "        test_input = torch.tensor([[int(bit) for bit in user_input]]).float()\n",
        "        with torch.no_grad():\n",
        "            encrypted_vector = model.encoder(test_input)\n",
        "            print('Encrypted vector:', encrypted_vector.numpy())\n",
        "\n",
        "            decrypted_vector = model.decoder(encrypted_vector)\n",
        "            binarized_output = torch.where(decrypted_vector > 0.5, torch.tensor(1.0), torch.tensor(0.0))\n",
        "            print('Binarized output:', binarized_output.numpy())\n",
        "\n",
        "    except ValueError:\n",
        "        print(\"Invalid input. Please ensure your input is an 8-bit binary sequence.\")\n",
        "\n",
        "def string_to_ascii_binary(input_string):\n",
        "    \"\"\"Convert a string to a list of 8-bit binary representations of its ASCII characters.\"\"\"\n",
        "    return [format(ord(c), '08b') for c in input_string]\n",
        "\n",
        "def binary_to_tensor(binary_list):\n",
        "    \"\"\"Convert a list of 8-bit binary strings to a tensor.\"\"\"\n",
        "    return torch.tensor([[int(bit) for bit in binary] for binary in binary_list], dtype=torch.float)\n",
        "\n",
        "def tensor_to_binary(tensor):\n",
        "    \"\"\"Convert a tensor to a list of binary strings.\"\"\"\n",
        "    return [''.join(str(int(bit)) for bit in sequence) for sequence in tensor]\n",
        "\n",
        "def binary_to_ascii(binary_list):\n",
        "    \"\"\"Convert a list of binary strings to their ASCII character equivalents.\"\"\"\n",
        "    return ''.join([chr(int(binary, 2)) for binary in binary_list])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model_path = '/content/drive/MyDrive/SecurityApplicationProject/hybrid_autoencoder_parameters.pth'\n",
        "    model = load_model(model_path)\n",
        "\n",
        "    user_input = input(\"Enter a passage in English: \")\n",
        "    ascii_binaries = string_to_ascii_binary(user_input)\n",
        "    input_tensor = binary_to_tensor(ascii_binaries)\n",
        "\n",
        "    encrypted_vectors = []\n",
        "    with torch.no_grad():\n",
        "        for sequence in input_tensor:\n",
        "            encrypted_vector = model.encoder(sequence.unsqueeze(0))\n",
        "            encrypted_vectors.append(encrypted_vector)\n",
        "\n",
        "    decrypted_vectors = []\n",
        "    for vector in encrypted_vectors:\n",
        "        decrypted_vector = model.decoder(vector)\n",
        "        binarized_output = torch.where(decrypted_vector > 0.5, torch.tensor(1.0), torch.tensor(0.0))\n",
        "        decrypted_vectors.append(binarized_output)\n",
        "\n",
        "    decrypted_binaries = tensor_to_binary(torch.cat(decrypted_vectors))\n",
        "    decrypted_text = binary_to_ascii(decrypted_binaries)\n",
        "\n",
        "    print(f\"Original Text: {user_input}\")\n",
        "    print(f\"Decrypted Text: {decrypted_text}\")\n"
      ],
      "metadata": {
        "id": "U-KmTQnjNiMz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bae7fc6-6418-4df6-bb17-1c3b259fa656"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a passage in English: IITJ\n",
            "Original Text: IITJ\n",
            "Decrypted Text: IITJ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encrypted_vectors_to_string(encrypted_vectors):\n",
        "    \"\"\"Convert encrypted vectors to a string representation.\"\"\"\n",
        "    # Convert each tensor in encrypted_vectors to a string of numbers separated by commas\n",
        "    # and join different vectors with a semicolon for readability\n",
        "    return '; '.join([', '.join([f'{element:.4f}' for element in vector.squeeze().tolist()]) for vector in encrypted_vectors])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model_path = '/content/drive/MyDrive/SecurityApplicationProject/hybrid_autoencoder_parameters.pth'\n",
        "    model = load_model(model_path)\n",
        "\n",
        "    user_input = input(\"Enter a passage in English: \")\n",
        "    ascii_binaries = string_to_ascii_binary(user_input)\n",
        "    input_tensor = binary_to_tensor(ascii_binaries)\n",
        "\n",
        "    encrypted_vectors = []\n",
        "    with torch.no_grad():\n",
        "        for sequence in input_tensor:\n",
        "            encrypted_vector = model.encoder(sequence.unsqueeze(0))\n",
        "            encrypted_vectors.append(encrypted_vector)\n",
        "\n",
        "    # Convert encrypted vectors to a string for display\n",
        "    encrypted_string = encrypted_vectors_to_string(encrypted_vectors)\n",
        "\n",
        "    decrypted_vectors = []\n",
        "    for vector in encrypted_vectors:\n",
        "        decrypted_vector = model.decoder(vector)\n",
        "        binarized_output = torch.where(decrypted_vector > 0.5, torch.tensor(1.0), torch.tensor(0.0))\n",
        "        decrypted_vectors.append(binarized_output)\n",
        "\n",
        "    decrypted_binaries = tensor_to_binary(torch.cat(decrypted_vectors))\n",
        "    decrypted_text = binary_to_ascii(decrypted_binaries)\n",
        "\n",
        "    print(f\"Original Text: {user_input}\")\n",
        "    print(f\"Encrypted (Vector Format): {encrypted_string}\")\n",
        "    print(f\"Decrypted Text: {decrypted_text}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8VehNl9THcs",
        "outputId": "144aa732-574a-4936-ffd3-f3117339bfe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a passage in English: IITJ\n",
            "Original Text: IITJ\n",
            "Encrypted (Vector Format): -2.0258, 2.2443, -1.5781, 1.3165, 2.6042, 4.7823, -1.7187, -1.1811; -2.0258, 2.2443, -1.5781, 1.3165, 2.6042, 4.7823, -1.7187, -1.1811; 5.0964, 0.2891, -5.8141, -3.8853, 2.7905, 2.7452, 2.6121, -1.4434; -3.6573, 3.2207, -2.3393, 0.0176, -2.4937, 5.7965, -6.2938, -2.9294\n",
            "Decrypted Text: IITJ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def string_to_encrypted_vectors(encrypted_string):\n",
        "    \"\"\"Parse the encrypted string back into a list of tensors.\"\"\"\n",
        "    vector_strings = encrypted_string.split('; ')\n",
        "    vectors = [torch.tensor([float(num) for num in vector.split(', ')], dtype=torch.float).unsqueeze(0) for vector in vector_strings]\n",
        "    return vectors\n",
        "\n",
        "def decrypt_encrypted_string(model, encrypted_string):\n",
        "    \"\"\"Decrypt an encrypted string back to text.\"\"\"\n",
        "    encrypted_vectors = string_to_encrypted_vectors(encrypted_string)\n",
        "    decrypted_vectors = []\n",
        "    with torch.no_grad():\n",
        "        for vector in encrypted_vectors:\n",
        "            decrypted_vector = model.decoder(vector)\n",
        "            binarized_output = torch.where(decrypted_vector > 0.5, torch.tensor(1.0), torch.tensor(0.0))\n",
        "            decrypted_vectors.append(binarized_output)\n",
        "    decrypted_binaries = tensor_to_binary(torch.cat(decrypted_vectors))\n",
        "    decrypted_text = binary_to_ascii(decrypted_binaries)\n",
        "    return decrypted_text\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model_path = '/content/drive/MyDrive/SecurityApplicationProject/hybrid_autoencoder_parameters.pth'\n",
        "    model = load_model(model_path)\n",
        "\n",
        "    # Assuming you have an encrypted string (from previous steps or stored separately)\n",
        "    encrypted_string = input(\"Encrypted string: \") # Replace with your actual encrypted string\n",
        "    decrypted_text = decrypt_encrypted_string(model, encrypted_string)\n",
        "\n",
        "    print(f\"Decrypted Text from Encrypted String: {decrypted_text}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7c3Z5gBkTesy",
        "outputId": "14bf7c44-fc9a-4048-e21d-0c6a724f1e81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encrypted string: -2.0258, 2.2443, -1.5781, 1.3165, 2.6042, 4.7823, -1.7187, -1.1811; -2.0258, 2.2443, -1.5781, 1.3165, 2.6042, 4.7823, -1.7187, -1.1811; 5.0964, 0.2891, -5.8141, -3.8853, 2.7905, 2.7452, 2.6121, -1.4434; -3.6573, 3.2207, -2.3393, 0.0176, -2.4937, 5.7965, -6.2938, -2.9294\n",
            "Decrypted Text from Encrypted String: IITJ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ioi7-cMlUfLM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}